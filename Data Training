class DisasterDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {
            key: torch.tensor(val[idx]) for key, val in self.encodings.items()
        }
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

train_dataset = DisasterDataset(
    encodings=tokenizer(df_train["text"].tolist(), truncation=True, padding=True),
    labels=df_train["target"].tolist()
)
dev_dataset = DisasterDataset(
    encodings=tokenizer(df_dev["text"].tolist(), truncation=True, padding=True),
    labels=df_dev["target"].tolist()
)
test_dataset = DisasterDataset(
    encodings=tokenizer(df_test["text"].tolist(), truncation=True, padding=True),
    labels=df_test["target"].tolist()
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=output_dir,
    evaluation_strategy=evaluation_strategy,
    save_total_limit=save_total_limit,
    save_strategy=save_strategy,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    learning_rate=learning_rate,
    warmup_steps=warmup_steps,
    weight_decay=weight_decay,
    metric_for_best_model=metric_for_best_model,
    report_to=report_to,
    load_best_model_at_end=load_best_model_at_end,
)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels = num_labels,
    ignore_mismatched_sizes = True
)

glue_metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    predictions = np.argmax(predictions, axis = 1)

    return glue_metric.compute(predictions = predictions, references = labels)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
)

trainer.train()
