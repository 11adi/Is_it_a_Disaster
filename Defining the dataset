train_df
train_df["target"].hist()
random_state = 42
df_train, df_remain = train_test_split(train_df,
                                       test_size=.4,
                                       random_state=random_state,
                                       stratify=train_df["target"])
df_dev, df_test = train_test_split(df_remain,
                                   test_size=.5,
                                   random_state=random_state,
                                   stratify=df_remain["target"])
df_train
df_dev
df_test
df_train = df_train[["text", "target"]]
df_dev = df_dev[["text", "target"]]
df_test = df_test[["text", "target"]]
df_train
df_train["target"].hist()
df_dev["target"].hist()
df_test["target"].hist()

# Now will start with the configuration of the model

tokenizer_name = "roberta-large"
model_name = "roberta-large"

num_labels = 2

# Saving
output_dir = "./results"
evaluation_strategy = "epoch"
save_strategy = "epoch"
save_total_limit = 2

# Training
num_train_epochs = 50
per_device_train_batch_size = 64
per_device_eval_batch_size = 64
learning_rate = 2e-5
warmup_steps = 500
weight_decay = 0.01
metric_for_best_model = "accuracy"
report_to = "wandb"
load_best_model_at_end = True

# Early stopping
early_stopping_patience = 5
